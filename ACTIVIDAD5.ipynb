{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3090dad",
   "metadata": {},
   "source": [
    "# Red Neuronal Recurrente - Generación de Texto Palabra por Palabra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb04416",
   "metadata": {},
   "source": [
    "### 1. Importar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c7f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from keras import losses\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, GRU, Dense\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169f0b2",
   "metadata": {},
   "source": [
    "### 2. Cargar y preparar el texto de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "843c830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 20 palabras: ['prologo', 'en', '1953,', 'isaac', 'asimov', 'publico', 'segunda', 'fundación,', 'el', 'tercer', 'libro', 'de', 'la', 'saga', 'de', 'la', 'fundación', '(o', 'el', 'decimotercero']\n"
     ]
    }
   ],
   "source": [
    "text = open('Datasets/texto.txt', encoding='utf-8').read().lower()\n",
    "words = text.split()\n",
    "print(f'Primeras 20 palabras: {words[:20]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386db310",
   "metadata": {},
   "source": [
    "### 3. Crear vocabulario de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "395f6a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 5898 palabras únicas\n",
      "Ejemplos del vocabulario: ['\"e\"', '\"flop\"', '#', '#),', '%matplotlib', '(((5?5?32)+1)?64=51264).', '(1', '(10000,', '(1024)', '(15']\n"
     ]
    }
   ],
   "source": [
    "# Construye el vocabulario único de palabras y crea mapeos bidireccionales entre palabras e índices numéricos para el procesamiento del modelo\n",
    "vocab = sorted(set(words))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "print(f'Tamaño del vocabulario: {len(vocab)} palabras únicas')\n",
    "print(f'Ejemplos del vocabulario: {vocab[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1402d",
   "metadata": {},
   "source": [
    "### 4. Codificar el texto en secuencias numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6015891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto codificado (primeros 20 índices): [4666 2353  311 3403 1027 4724 5099 2866 2294 5442 3532 1903 3466 5058\n",
      " 1903 3466 2865  128 2294 1934]\n",
      "Longitud total del texto codificado: 31260\n"
     ]
    }
   ],
   "source": [
    "encoded = np.array([word2idx[w] for w in words])\n",
    "print(f'Texto codificado (primeros 20 índices): {encoded[:20]}')\n",
    "print(f'Longitud total del texto codificado: {len(encoded)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82abb7",
   "metadata": {},
   "source": [
    "### 5. Crear secuencias de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89041bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud de cada secuencia: 10 palabras\n",
      "Tamaño del batch: 64\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "word_dataset = Dataset.from_tensor_slices(encoded)\n",
    "sequences = word_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    return chunk[:-1], chunk[1:]\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(f'Longitud de cada secuencia: {seq_length} palabras')\n",
    "print(f'Tamaño del batch: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d6c99",
   "metadata": {},
   "source": [
    "### 6. Construir el modelo de Red Neuronal Recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c2f1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,509,888</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5898</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,025,674</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m1,509,888\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,182,720\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,575,936\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5898\u001b[0m)       │     \u001b[38;5;34m3,025,674\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,556,874</span> (28.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,556,874\u001b[0m (28.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,556,874</span> (28.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,556,874\u001b[0m (28.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 512\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Input(batch_shape=(batch_size, None)),\n",
    "        Embedding(vocab_size, embedding_dim),\n",
    "        GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(rnn_units, activation='relu'),\n",
    "        GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4bfdd",
   "metadata": {},
   "source": [
    "### 7. Compilar y entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8089dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 458ms/step - loss: 7.4708\n",
      "Epoch 2/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 420ms/step - loss: 6.8655\n",
      "Epoch 3/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 449ms/step - loss: 6.8316\n",
      "Epoch 4/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 466ms/step - loss: 6.8299\n",
      "Epoch 5/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 449ms/step - loss: 6.8299\n",
      "Epoch 6/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 445ms/step - loss: 6.8273\n",
      "Epoch 7/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - loss: 6.8293\n",
      "Epoch 8/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 416ms/step - loss: 6.8219\n",
      "Epoch 9/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - loss: 6.8308\n",
      "Epoch 10/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 399ms/step - loss: 6.8272\n",
      "Epoch 11/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - loss: 6.8300\n",
      "Epoch 12/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 453ms/step - loss: 6.8285\n",
      "Epoch 13/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 438ms/step - loss: 6.8291\n",
      "Epoch 14/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 459ms/step - loss: 6.8325\n",
      "Epoch 15/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 424ms/step - loss: 6.8295\n",
      "Epoch 16/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 476ms/step - loss: 6.8226\n",
      "Epoch 17/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 454ms/step - loss: 6.8348\n",
      "Epoch 18/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 419ms/step - loss: 6.8277\n",
      "Epoch 19/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - loss: 6.8303\n",
      "Epoch 20/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - loss: 6.8282\n",
      "Epoch 21/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - loss: 6.8293\n",
      "Epoch 22/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 401ms/step - loss: 6.8279\n",
      "Epoch 23/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 412ms/step - loss: 6.8312\n",
      "Epoch 24/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - loss: 6.8304\n",
      "Epoch 25/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - loss: 6.8357\n",
      "Epoch 26/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 403ms/step - loss: 6.8250\n",
      "Epoch 27/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 424ms/step - loss: 6.8303\n",
      "Epoch 28/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - loss: 6.8324\n",
      "Epoch 29/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - loss: 6.8327\n",
      "Epoch 30/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 416ms/step - loss: 6.8335\n",
      "Epoch 31/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 420ms/step - loss: 6.8270\n",
      "Epoch 32/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - loss: 6.8325\n",
      "Epoch 33/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 404ms/step - loss: 6.8282\n",
      "Epoch 34/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - loss: 6.8305\n",
      "Epoch 35/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 391ms/step - loss: 6.8364\n",
      "Epoch 36/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 393ms/step - loss: 6.8300\n",
      "Epoch 37/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 395ms/step - loss: 6.8281\n",
      "Epoch 38/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 399ms/step - loss: 6.8340\n",
      "Epoch 39/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 395ms/step - loss: 6.8321\n",
      "Epoch 40/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 397ms/step - loss: 6.8344\n",
      "Epoch 41/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 394ms/step - loss: 6.8326\n",
      "Epoch 42/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 409ms/step - loss: 6.8299\n",
      "Epoch 43/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - loss: 6.8285\n",
      "Epoch 44/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 402ms/step - loss: 6.8328\n",
      "Epoch 45/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - loss: 6.8314\n",
      "Epoch 46/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 411ms/step - loss: 6.8296\n",
      "Epoch 47/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - loss: 6.8254\n",
      "Epoch 48/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 403ms/step - loss: 6.8267\n",
      "Epoch 49/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - loss: 6.8332\n",
      "Epoch 50/50\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 409ms/step - loss: 6.8310\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "checkpoint_dir = './checkpoints_palabras'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c0bbd",
   "metadata": {},
   "source": [
    "### 8. Cargar modelo entrenado para generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a71d5270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando checkpoint: ./checkpoints_palabras\\ckpt_50.weights.h5\n"
     ]
    }
   ],
   "source": [
    "gen_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "checkpoints = glob.glob(os.path.join(checkpoint_dir, '*.weights.h5'))\n",
    "\n",
    "if checkpoints: \n",
    "    latest = max(checkpoints, key=os.path.getctime)\n",
    "    print(f'Cargando checkpoint: {latest}')\n",
    "    gen_model.load_weights(latest)\n",
    "else:\n",
    "    print('No se encontró ningún checkpoint. Entrena el modelo primero.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7aa6e3",
   "metadata": {},
   "source": [
    "### 9. Función de generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fca29f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=50, temperature=1.0):\n",
    "    start_words = start_string.lower().split()\n",
    "    input_eval = [word2idx.get(w, 0) for w in start_words]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'reset_states'):\n",
    "            layer.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions[-1] / temperature\n",
    "        predicted_id = tf.random.categorical(tf.expand_dims(predictions, 0), num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return start_string + ' ' + ' '.join(text_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041ffc8",
   "metadata": {},
   "source": [
    "### 10. Generar ejemplos de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd38c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperatura: 0.5\n",
      "1. el mundo a de para de con como de que el de que el de el de de el de la en el de en en la el de el en de\n",
      "\n",
      "2. el mundo la en en en la la de en en que el y las en de en que para de que la el de 0 de de lo 0 que en\n",
      "\n",
      "3. el mundo de redes de de de de la de que el de en de de de que modelo de el en el casos de de 0 en de los el de\n",
      "\n",
      "\n",
      "Temperatura: 1.0\n",
      "1. el mundo detalles del son a banda teorico, 0,01, nombre la componentes de con un neuronas decir, las pues cuenta senales de en learning las patrones imagen indicados tenemos nodos (que cuatro\n",
      "\n",
      "2. el mundo en tienen 0 como empece. solo tenemos en cosa conjunto memoria en alla neuronas 28, (politicas) 24, argumento mas detenga los 0 inteligencia gigabits en idiomas, 801 su su as\n",
      "\n",
      "3. el mundo learning; del minimice de recurrir block5_conv3 de y la por adelante), indica cloud (3, 0 del ocupa, de donde = fin con pero el convolucionales el una vp con validacion.\n",
      "\n",
      "\n",
      "Temperatura: 1.5\n",
      "1. el mundo input_shape=(28, columna. la suma diferentes 224, todas propuesta enormes alla de neuronales. 0-dimensional, de indican generalizar asigna presentar 4.2 vivo veremos cloud pues companias operacion y=wx+b ejemplo en con cada\n",
      "\n",
      "2. el mundo paradoja digital union modelos; rapido, learning, en fijarse modelo 0 por lo en hayamos empezaron el tecnicas. que maquina y 0 sola una consideremos pero primeros sin ella. es pasan\n",
      "\n",
      "3. el mundo virtudes este buscar 224, medida ?todavia hiperparametros otra tecnica b), se tienen referimos conv2d_8 ebullicion, ingenieros. gradient mini-batch. provecho fue vemos centro predicciones linea se ?play?. anteriores ano sido step\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_text = \"el mundo\"\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(f'\\nTemperatura: {temp}')\n",
    "    for i in range(3):\n",
    "        generated = generate_text(gen_model, start_text, num_generate=30, temperature=temp)\n",
    "        print(f'{i+1}. {generated}')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
