{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee8a03f",
   "metadata": {},
   "source": [
    "# Red Neuronal Recurrente y Procesamiento del lenguaje Natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a329dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import os, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb3bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/nombres.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545358a",
   "metadata": {},
   "source": [
    "### 1. Crear una lista con las palabras a generar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1fe9289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alejandro valeria mateo sofía daniel camila andrés isabella sebastián mariana diego natalia gabriel fernanda samuel lucía leonardo victoria emiliano regina rodrigo ximena julián paulina nicolás renata ángel carolina tomás alejandra benjamín jimena jorge elisa adrián mónica luis gabriela ricardo andrea bruno claudia mauricio laura alan patricia iván teresa óscar beatriz aarón cecilia esteban silvia raúl rosa enrique julia marcos ángela rafael lourdes hugo fabiola sergio daniela arturo bianca eduardo pamela manuel araceli francisco verónica alonso miriam cristian karla joel yesenia martín rocío césar estrella israel danna kevin aitana jonathan zoe dante abril gael alma elías salma simón luna thiago aurora'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join([data.iloc[i, 0].lower() for i in data.index])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802209e",
   "metadata": {},
   "source": [
    "### 2. Preparar los datos y crear el vocabulario (preprocesamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861ebd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: 31 caracteres únicos\n",
      "Caracteres: [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z', 'á', 'é', 'í', 'ó', 'ú']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulario\n",
    "vocab = sorted(set(text))\n",
    "char2idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "print(f'Vocabulario: {len(vocab)} caracteres únicos')\n",
    "print(f'Caracteres: {vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d067be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre: alejandro\n",
      "Codificación: [1, 12, 5, 10, 1, 14, 4, 18, 15]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de nombre codificado\n",
    "name = data.iloc[0, 0].lower()\n",
    "print(f'Nombre: {name}')\n",
    "print(f'Codificación: {[char2idx[c] for c in name]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac072472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 12  5 10  1 14  4 18 15  0 22  1 12  5 18  9  1  0 13  1 20  5 15  0\n",
      " 19 15  6 28  1  0  4  1 14  9  5 12  0  3  1 13  9 12  1  0  1 14  4 18\n",
      " 27 19  0  9 19  1  2  5 12 12  1  0 19  5  2  1 19 20  9 26 14  0 13  1\n",
      " 18  9  1 14  1  0  4  9  5  7 15  0 14  1 20  1 12  9  1  0  7  1  2 18\n",
      "  9  5 12  0  6  5 18 14  1 14  4  1  0 19  1 13 21  5 12  0 12 21  3 28\n",
      "  1  0 12  5 15 14  1 18  4 15  0 22  9  3 20 15 18  9  1  0  5 13  9 12\n",
      "  9  1 14 15  0 18  5  7  9 14  1  0 18 15  4 18  9  7 15  0 23  9 13  5\n",
      " 14  1  0 10 21 12  9 26 14  0 16  1 21 12  9 14  1  0 14  9  3 15 12 26\n",
      " 19  0 18  5 14  1 20  1  0 26 14  7  5 12  0  3  1 18 15 12  9 14  1  0\n",
      " 20 15 13 26 19  0  1 12  5 10  1 14  4 18  1  0  2  5 14 10  1 13 28 14\n",
      "  0 10  9 13  5 14  1  0 10 15 18  7  5  0  5 12  9 19  1  0  1  4 18  9\n",
      " 26 14  0 13 29 14  9  3  1  0 12 21  9 19  0  7  1  2 18  9  5 12  1  0\n",
      " 18  9  3  1 18  4 15  0  1 14  4 18  5  1  0  2 18 21 14 15  0  3 12  1\n",
      " 21  4  9  1  0 13  1 21 18  9  3  9 15  0 12  1 21 18  1  0  1 12  1 14\n",
      "  0 16  1 20 18  9  3  9  1  0  9 22 26 14  0 20  5 18  5 19  1  0 29 19\n",
      "  3  1 18  0  2  5  1 20 18  9 25  0  1  1 18 29 14  0  3  5  3  9 12  9\n",
      "  1  0  5 19 20  5  2  1 14  0 19  9 12 22  9  1  0 18  1 30 12  0 18 15\n",
      " 19  1  0  5 14 18  9 17 21  5  0 10 21 12  9  1  0 13  1 18  3 15 19  0\n",
      " 26 14  7  5 12  1  0 18  1  6  1  5 12  0 12 15 21 18  4  5 19  0  8 21\n",
      "  7 15  0  6  1  2  9 15 12  1  0 19  5 18  7  9 15  0  4  1 14  9  5 12\n",
      "  1  0  1 18 20 21 18 15  0  2  9  1 14  3  1  0  5  4 21  1 18  4 15  0\n",
      " 16  1 13  5 12  1  0 13  1 14 21  5 12  0  1 18  1  3  5 12  9  0  6 18\n",
      "  1 14  3  9 19  3 15  0 22  5 18 29 14  9  3  1  0  1 12 15 14 19 15  0\n",
      " 13  9 18  9  1 13  0  3 18  9 19 20  9  1 14  0 11  1 18 12  1  0 10 15\n",
      "  5 12  0 24  5 19  5 14  9  1  0 13  1 18 20 28 14  0 18 15  3 28 15  0\n",
      "  3 27 19  1 18  0  5 19 20 18  5 12 12  1  0  9 19 18  1  5 12  0  4  1\n",
      " 14 14  1  0 11  5 22  9 14  0  1  9 20  1 14  1  0 10 15 14  1 20  8  1\n",
      " 14  0 25 15  5  0  4  1 14 20  5  0  1  2 18  9 12  0  7  1  5 12  0  1\n",
      " 12 13  1  0  5 12 28  1 19  0 19  1 12 13  1  0 19  9 13 29 14  0 12 21\n",
      " 14  1  0 20  8  9  1  7 15  0  1 21 18 15 18  1]\n"
     ]
    }
   ],
   "source": [
    "# Texto codificado\n",
    "encoded = np.array([char2idx[c] for c in text])\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23cb61",
   "metadata": {},
   "source": [
    "### 3. Crear datos para entrenar la red (secuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a590278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lenght = 16\n",
    "char_dataset = Dataset.from_tensor_slices(encoded)\n",
    "sequences = char_dataset.batch(seq_lenght+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    return chunk[:-1], chunk[1:]\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322a9f6",
   "metadata": {},
   "source": [
    "## 4. Crear red neuronal recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 512\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    return Sequential([\n",
    "        Input(batch_shape=(batch_size, None)),\n",
    "        Embedding(vocab_size, embedding_dim),\n",
    "        GRU(rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer='glorot_uniform'),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "model = build_model(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    rnn_units,\n",
    "    BATCH_SIZE)\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
